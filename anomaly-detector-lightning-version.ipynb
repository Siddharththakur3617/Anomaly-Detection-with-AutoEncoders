{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3209332,"sourceType":"datasetVersion","datasetId":1946896}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"e10b93fe","cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import transforms, datasets\nimport torch.nn as nn\nfrom torchmetrics import Accuracy\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom sklearn.model_selection import train_test_split\nfrom torchsummary import summary\nfrom torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n\nimport os\nfrom torchvision.transforms import ToPILImage\nfrom tqdm import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nimport pytorch_lightning as pl\nfrom dataclasses import dataclass\nfrom typing import Tuple\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:14:02.966666Z","iopub.execute_input":"2025-06-11T04:14:02.967266Z","iopub.status.idle":"2025-06-11T04:14:18.571188Z","shell.execute_reply.started":"2025-06-11T04:14:02.967235Z","shell.execute_reply":"2025-06-11T04:14:18.570643Z"}},"outputs":[],"execution_count":1},{"id":"31aff47a","cell_type":"code","source":"@dataclass\nclass Configure:\n    \"\"\"\n    Args:\n        data_dir : The path to the directory where the MNIST dataset is stored. Defaults to the value of\n            the 'PATH_DATASETS' environment variable or '.' if not set.\n\n        batch_size : The batch size to use during training. Defaults to 256 if a GPU is available,\n            or 64 otherwise.\n\n        max_epochs : The maximum number of epochs to train the model for. Defaults to 3.\n\n        accelerator : The accelerator to use for training. Can be one of \"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\".\n\n        devices : The number of devices to use for training. Defaults to 1.\n    \"\"\"\n\n    data_dir : str = '/kaggle/input/mvtec-ad/bottle/train'\n    # writing_dir : str = '/kaggle/working'\n    save_dir : str = '/kaggle/working/augmented_data'\n    test_dir : str = '/kaggle/input/mvtec-ad/bottle/test'\n    batch_size : int = 32 if torch.cuda.is_available() else 8\n    max_epochs : int = 51\n    accelerator : str = 'auto'\n    devices : int = 1\n\nconfig = Configure()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:14:18.572381Z","iopub.execute_input":"2025-06-11T04:14:18.572797Z","iopub.status.idle":"2025-06-11T04:14:18.577780Z","shell.execute_reply.started":"2025-06-11T04:14:18.572767Z","shell.execute_reply":"2025-06-11T04:14:18.576993Z"}},"outputs":[],"execution_count":2},{"id":"d392834c","cell_type":"code","source":"class AnomalyDetector(pl.LightningModule):\n\n    def __init__(self, data_dir : str = config.data_dir, # writing_dir : str = config.writing_dir, \n                 save_dir : str = config.save_dir,\n                 test_dir : str = config.test_dir, accelerator : str = config.accelerator, \n                 learning_rate : float = 0.01, alpha : float = 0.55, beta : float = 0.0, \n                 gamma : float = 0.45, validation_size : float = 0.6, \n                 batch_size : int = config.batch_size, num_workers : int = 2, \n                 weight_decay : float = 0.0, threshold_percentile : int = 95):\n\n        super().__init__()\n\n        self.data_dir = data_dir   \n        # self.writing_dir = writing_dir  \n        self.save_dir = save_dir\n        self.test_dir = test_dir\n        self.learning_rate = learning_rate\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.val_size = validation_size\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.weight_decay = weight_decay\n        self.threshold_percentile = threshold_percentile\n        if accelerator == 'auto':\n            self.my_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        else:\n            self.my_device = torch.device(accelerator)\n\n        self.val_accuracy = Accuracy(task='multiclass', num_classes=2)\n        self.test_accuracy = Accuracy(task='multiclass', num_classes=2)\n        self.pixel_errors_list = []\n        self.pixel_threshold = 0\n        self.example_input_image = None\n        self.example_recon_image = None\n\n        self.transform = torchvision.transforms.Compose([\n            transforms.Resize((256,256)),\n            transforms.ToTensor()\n        ])\n\n        self.augment_transform = torchvision.transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n        ])\n        \n        self.Encoder = nn.Sequential(\n            \n            nn.Conv2d(3, 8, kernel_size=3, padding='same'),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),   \n            nn.Dropout(p=.3),                        # (3x256x256) -> (8x128x128)\n\n            nn.Conv2d(8, 16, kernel_size=3, padding='same'),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(p=.3),                       # (8x128x128) -> (16x64x64)\n\n            nn.Conv2d(16, 32, kernel_size=3, padding='same'),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),   \n            nn.Dropout(p=.3),                      # (16x64x64) -> (32x32x32)\n\n            nn.Conv2d(32, 32, kernel_size=3, padding='same'),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(p=.3),                      # (32x32x32) -> (32x16x16)\n            \n            nn.Flatten()\n        )\n    \n        self.Bottleneck = nn.Sequential(\n            \n            nn.Linear(32*16*16, 1024),\n            nn.ReLU(),\n            # nn.Dropout(p=.3),\n            \n            nn.Linear(1024, 1024),\n            nn.ReLU(),\n            # nn.Dropout(p=.3),\n            \n            nn.Linear(1024, 32*16*16),\n            nn.ReLU(),\n            # nn.Dropout(p=.3),\n            \n            nn.Unflatten(-1,(32,16,16))\n        )\n\n        self.Decoder = nn.Sequential(\n            \n            nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1, output_padding=0),\n            nn.ReLU(),\n            # nn.Dropout(p=.3),       # (32x16x16) -> (32x32x32)\n            \n            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1, output_padding=0),\n            nn.ReLU(),\n            # nn.Dropout(p=.3),       # (32x32x32) -> (16x64x64)\n\n            nn.ConvTranspose2d(16, 8, kernel_size=4, stride=2, padding=1, output_padding=0),\n            nn.ReLU(),\n            # nn.Dropout(p=.3),       # (16x64x64) -> (8x128x128)\n\n            nn.ConvTranspose2d(8, 3, kernel_size=4, stride=2, padding=1, output_padding=0),\n            nn.Sigmoid()\n            # nn.ReLU(),\n            # nn.Dropout(p=.3)        # (8x128x128) -> (3x256x256)\n        )\n\n    def forward(self, x : torch.tensor) -> torch.tensor:\n        x = self.Encoder(x)\n        x = self.Bottleneck(x)\n        x = self.Decoder(x)\n        return x\n\n    def data_augmentation(self) -> None:\n        num_augs = 5  # Number of augmented versions per image\n        dataset = datasets.ImageFolder(root=self.data_dir)\n        # Save augmentations\n        for idx, (img, label) in tqdm(enumerate(dataset), total=len(dataset)):\n            class_name = dataset.classes[label]\n            input_path = dataset.imgs[idx][0]\n            # Output class folder\n            class_out_dir = os.path.join(self.save_dir, class_name)\n            os.makedirs(class_out_dir, exist_ok=True)\n\n            for i in range(num_augs):\n               aug_img = self.augment_transform(img)\n               aug_img = ToPILImage()(transforms.ToTensor()(aug_img))  # Reconvert safely to PIL\n               base_name = os.path.basename(input_path).split('.')[0]\n               save_path = os.path.join(class_out_dir, f\"{base_name}_aug{i}.jpg\")\n               aug_img.save(save_path)\n\n    def compute_loss(self, reconstruction, actual, reduction : str = 'none'):\n        \n        batch_size = actual.size(0)\n        ssim_loss_fn = StructuralSimilarityIndexMeasure(data_range=1.0).to(self.my_device)\n        ssim_vals = torch.zeros(batch_size, device = self.my_device)\n\n        for i in range(batch_size):\n            ssim_vals[i] = 1 - ssim_loss_fn(reconstruction[i].unsqueeze(0), actual[i].unsqueeze(0))\n            \n        mse_vals = torch.mean(nn.functional.mse_loss(reconstruction, actual, reduction='none'), dim=[1,2,3])\n        l1_vals = torch.mean(nn.functional.l1_loss(reconstruction, actual, reduction='none'), dim=[1,2,3])\n        \n        total = self.alpha * ssim_vals + self.beta * mse_vals + self.gamma * l1_vals\n        \n        if reduction == 'mean':\n            return total.mean()\n        return total\n\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate)\n        scheduler = {\n            'scheduler' : torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.5),\n            'monitor' : 'Validation accuracy',\n            'interval' : 'epoch',\n            'frequency' : 1\n        }\n        return {'optimizer':optimizer, 'lr_scheduler':scheduler}\n\n    def prepare_data(self) -> None:\n        torchvision.datasets.ImageFolder(root = self.data_dir, transform = self.transform)\n        torchvision.datasets.ImageFolder(root = self.save_dir, transform = self.transform)\n        torchvision.datasets.ImageFolder(root = self.test_dir, transform=self.transform)\n\n    def setup(self, stage : str = None) -> None:\n\n        raw_dataset = torchvision.datasets.ImageFolder(root = self.data_dir, transform = self.transform)\n        augment_dataset = torchvision.datasets.ImageFolder(root = self.save_dir, transform = self.transform)\n        self.train_dataset = ConcatDataset([raw_dataset, augment_dataset])\n\n        label_remap = lambda y: 0 if y < 3 else 1\n        dataset = torchvision.datasets.ImageFolder(root=self.test_dir, transform=self.transform, target_transform=label_remap)\n        self.test_dataset, self.val_dataset = train_test_split(dataset, test_size=self.val_size)\n\n    def training_step(self, batch : Tuple[torch.tensor, torch.tensor], batch_idx : int) -> torch.tensor:\n        actual, _ = batch\n        reconstructed = self(actual)\n\n        if batch_idx==0 and self.example_input_image == None:\n            self.example_input_image = actual[0].detach().cpu()\n            self.example_recon_image = reconstructed[0].detach().cpu()\n        \n        loss = self.compute_loss(reconstructed, actual, reduction='mean')\n        self.log('Training loss', loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch : Tuple[torch.tensor, torch.tensor], batch_idx : int) -> None:\n        actual, y = batch\n        reconstructed = self(actual)\n        pixel_errors = (reconstructed - actual) ** 2\n        self.pixel_errors_list.append(pixel_errors.cpu())\n        loss = self.compute_loss(reconstructed, actual)\n        flags = (loss < self.pixel_threshold).int()\n        self.val_accuracy.update(flags, y)\n        self.log('Validation accuracy', self.val_accuracy, prog_bar=True)\n\n    def test_step(self, batch : Tuple[torch.tensor, torch.tensor], batch_idx : int) -> None:\n        actual, y = batch\n        reconstructed = self(actual)\n        loss = self.compute_loss(reconstructed, actual, reduction='none')\n        flags = (loss < self.pixel_threshold).int()\n        self.test_accuracy.update(flags, y)\n\n    def on_train_epoch_start(self):\n        self.example_input_image = None\n        self.example_recon_image = None\n    \n    def on_train_epoch_end(self):\n        if self.example_input_image is not None:\n\n            input_img_np = self.example_input_image.permute(1,2,0).numpy()  # (C,H,W) -> (H,W,C)\n            recon_img_np = self.example_recon_image.permute(1,2,0).numpy() \n\n            input_img = wandb.Image(input_img_np, caption='Input Image')\n            recon_img = wandb.Image(recon_img_np, caption='Reconstructed Image')\n\n            self.logger.experiment.log({\n                'Input vs Reconstruction' : [input_img, recon_img],\n                'Epoch' : self.current_epoch\n            })\n    \n    def on_validation_epoch_end(self):\n        all_pixel_errors = torch.cat(self.pixel_errors_list, dim=0)\n        all_pixel_errors = all_pixel_errors.numpy().flatten()\n\n        self.pixel_threshold = np.percentile(all_pixel_errors, self.threshold_percentile)\n        self.pixel_errors_list.clear()\n    \n    def train_dataloader(self) -> DataLoader:\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, pin_memory=True)\n    \n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True)\n    \n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:18:22.005176Z","iopub.execute_input":"2025-06-11T04:18:22.005768Z","iopub.status.idle":"2025-06-11T04:18:22.031635Z","shell.execute_reply.started":"2025-06-11T04:18:22.005740Z","shell.execute_reply":"2025-06-11T04:18:22.030911Z"}},"outputs":[],"execution_count":10},{"id":"7af784f0-14bf-41e6-9794-3e9eb2367dc9","cell_type":"code","source":"user_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"wandb_api_key\")\nos.environ[\"WANDB_API_KEY\"] = secret_value\nwandb.login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:18:47.971253Z","iopub.execute_input":"2025-06-11T04:18:47.971926Z","iopub.status.idle":"2025-06-11T04:18:48.071039Z","shell.execute_reply.started":"2025-06-11T04:18:47.971900Z","shell.execute_reply":"2025-06-11T04:18:48.070495Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":12},{"id":"185d0aef-da1f-4d06-863f-e932b5a8be74","cell_type":"code","source":"early_stop_callback = EarlyStopping(\n    monitor = 'Validation accuracy',\n    min_delta = 0.01,\n    patience = 5,\n    verbose = 1,\n    mode = 'max'\n)\n\ncheckpoint_callback = ModelCheckpoint(dirpath = '/kaggle/working/checkpoints', save_top_k=1, monitor = 'Validation accuracy', mode = 'max')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:18:52.000009Z","iopub.execute_input":"2025-06-11T04:18:52.000312Z","iopub.status.idle":"2025-06-11T04:18:52.005302Z","shell.execute_reply.started":"2025-06-11T04:18:52.000278Z","shell.execute_reply":"2025-06-11T04:18:52.004742Z"}},"outputs":[],"execution_count":13},{"id":"448ed772","cell_type":"code","source":"wandb_logger = WandbLogger(project=\"Anomaly Detector\")\nmodel = AnomalyDetector()\ntrainer = pl.Trainer(\n        accelerator=config.accelerator,\n        devices=config.devices,\n        max_epochs=config.max_epochs,\n        logger=wandb_logger,\n        callbacks = [early_stop_callback, checkpoint_callback]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:18:56.129623Z","iopub.execute_input":"2025-06-11T04:18:56.130132Z","iopub.status.idle":"2025-06-11T04:18:56.317880Z","shell.execute_reply.started":"2025-06-11T04:18:56.130107Z","shell.execute_reply":"2025-06-11T04:18:56.317356Z"}},"outputs":[],"execution_count":14},{"id":"7d14e529-c007-415f-90eb-2689a4ca8fe0","cell_type":"code","source":"model.data_augmentation()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:14:24.942984Z","iopub.execute_input":"2025-06-11T04:14:24.943156Z","iopub.status.idle":"2025-06-11T04:16:04.852437Z","shell.execute_reply.started":"2025-06-11T04:14:24.943141Z","shell.execute_reply":"2025-06-11T04:16:04.851706Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 209/209 [01:39<00:00,  2.10it/s]\n","output_type":"stream"}],"execution_count":7},{"id":"07cc37fd-bc07-4b32-b2e5-78dd4b0bfa54","cell_type":"code","source":"trainer.fit(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:19:04.132501Z","iopub.execute_input":"2025-06-11T04:19:04.133208Z","iopub.status.idle":"2025-06-11T04:20:16.540218Z","shell.execute_reply.started":"2025-06-11T04:19:04.133183Z","shell.execute_reply":"2025-06-11T04:20:16.539654Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>./wandb/run-20250611_041904-kpysgn7t</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/siddharthofficial2014-indian-institute-of-technology-indore/Anomaly%20Detector/runs/kpysgn7t' target=\"_blank\">playful-puddle-8</a></strong> to <a href='https://wandb.ai/siddharthofficial2014-indian-institute-of-technology-indore/Anomaly%20Detector' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/siddharthofficial2014-indian-institute-of-technology-indore/Anomaly%20Detector' target=\"_blank\">https://wandb.ai/siddharthofficial2014-indian-institute-of-technology-indore/Anomaly%20Detector</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/siddharthofficial2014-indian-institute-of-technology-indore/Anomaly%20Detector/runs/kpysgn7t' target=\"_blank\">https://wandb.ai/siddharthofficial2014-indian-institute-of-technology-indore/Anomaly%20Detector/runs/kpysgn7t</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/checkpoints exists and is not empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (40) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1be3a11aadad44c9837f1f298863dea6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":15},{"id":"5671dc3d-0bef-47dd-88cb-5806fa39ca40","cell_type":"code","source":"model.pixel_threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:20:40.861577Z","iopub.execute_input":"2025-06-11T04:20:40.861891Z","iopub.status.idle":"2025-06-11T04:20:40.868070Z","shell.execute_reply.started":"2025-06-11T04:20:40.861869Z","shell.execute_reply":"2025-06-11T04:20:40.867494Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0.037598684430122375"},"metadata":{}}],"execution_count":16},{"id":"22ec2284-748e-48ec-b870-e121ba7c2db3","cell_type":"code","source":"trainer.test(model, dataloaders=model.val_dataloader(), verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:20:47.388093Z","iopub.execute_input":"2025-06-11T04:20:47.388355Z","iopub.status.idle":"2025-06-11T04:20:50.285495Z","shell.execute_reply.started":"2025-06-11T04:20:47.388336Z","shell.execute_reply":"2025-06-11T04:20:50.284927Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"328db26d243b43b88b2bccdefdb977d6"}},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[{}]"},"metadata":{}}],"execution_count":17},{"id":"b62e5e27-0c41-4c8d-a76d-279dd477fce1","cell_type":"code","source":"model.test_accuracy.compute()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:20:52.798418Z","iopub.execute_input":"2025-06-11T04:20:52.798812Z","iopub.status.idle":"2025-06-11T04:20:52.816395Z","shell.execute_reply.started":"2025-06-11T04:20:52.798787Z","shell.execute_reply":"2025-06-11T04:20:52.815897Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"tensor(0.8800)"},"metadata":{}}],"execution_count":18},{"id":"a87f8f1c-f002-4406-aa74-d03cff48ae64","cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:20:57.155021Z","iopub.execute_input":"2025-06-11T04:20:57.155500Z","iopub.status.idle":"2025-06-11T04:20:57.730345Z","shell.execute_reply.started":"2025-06-11T04:20:57.155478Z","shell.execute_reply":"2025-06-11T04:20:57.729836Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>Training loss</td><td>█▃▁▁</td></tr><tr><td>Validation accuracy</td><td>█▁▁███</td></tr><tr><td>epoch</td><td>▁▂▂▄▄▅▅▇▇█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▃▄▄▅▅▅▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Training loss</td><td>0.15551</td></tr><tr><td>Validation accuracy</td><td>0.88</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>trainer/global_step</td><td>239</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">playful-puddle-8</strong> at: <a href='https://wandb.ai/siddharthofficial2014-indian-institute-of-technology-indore/Anomaly%20Detector/runs/kpysgn7t' target=\"_blank\">https://wandb.ai/siddharthofficial2014-indian-institute-of-technology-indore/Anomaly%20Detector/runs/kpysgn7t</a><br> View project at: <a href='https://wandb.ai/siddharthofficial2014-indian-institute-of-technology-indore/Anomaly%20Detector' target=\"_blank\">https://wandb.ai/siddharthofficial2014-indian-institute-of-technology-indore/Anomaly%20Detector</a><br>Synced 5 W&B file(s), 12 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250611_041904-kpysgn7t/logs</code>"},"metadata":{}}],"execution_count":19}]}